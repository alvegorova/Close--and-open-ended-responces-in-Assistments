rm(list = ls())
install.packages("tidyverse")
require(ggplot2)
require(dplyr)
require(tidyr)
library(readxl) #for excel

setwd("~/Documents/Data_Analysis/EduAI_class")

#### FULL DATA
full_data <- read.csv("final_ready_for_analysis2.csv")
View(full_data)

###Choose important variables
colnames(full_data)
full_data <- full_data %>% dplyr::select (assignment_log_id, assignment_xid, user_xid, 
                                      prior_subparts_count, prior_subparts_avg_score_avg, 
                                      prior_knowledge_continuous, ans_body_clean2, ans_body_length)

###Descriptive statistics

#Variable "success in subparts"
hist(full_data$prior_subparts_avg_score_avg)
shapiro.test (full_data$prior_subparts_avg_score_avg)

#Variable "prior knowledge"
hist(full_data$prior_knowledge_continuous)
shapiro.test (full_data$prior_knowledge_continuous)

#Data with prior subparts
data_without_0 <- full_data %>% filter (prior_subparts_avg_score_avg>0)
hist(data_without_0$prior_subparts_avg_score_avg) 

#Data with more than 1 prior subparts
data_more_1_prior_parts <- full_data %>% filter (prior_subparts_count>1)
hist(data_more_1_prior_parts$prior_subparts_avg_score_avg) 

#Data with success in subparts more than 0.5
data_av_score_less_05 <- full_data %>% filter (prior_subparts_avg_score_avg %in% c(0.1, 0.2, 0.3, 0.4))
View(data_av_score_less_05)
sum(data_av_score_less_05$ans_body_clean2 !="")

#Data with success in subparts 0
data_av_score_0 <- full_data %>% filter (prior_subparts_avg_score_avg == 0)
View(data_av_score_0)

#Relationships between variables "Success in subparts" and "Prior knowledge"
plot(full_data$prior_knowledge_continuous, full_data$prior_subparts_avg_score_avg)

#### OUR DATASET
dataset <- read_excel("~/Documents/Data_Analysis/EduAI_class/survey_questions_dataset_v6.1.2.xlsx")
View(dataset)
str(dataset)

###Choose important variables
colnames(dataset)
names(dataset)[names(dataset) == '3rd_rater_engaged'] <- 'rater_3_engagement'
dataset <- dataset %>% dplyr::select (assignment_log_id, assignment_xid, user_xid, 
                                      prior_subparts_count, prior_subparts_avg_score_avg, 
                                      prior_knowledge_continuous, ans_body_clean2, ans_body_length,
                                      Question_in_survey, rater_1_engagement, rater_2_engagement, rater_3_engagement, rater_1_challenge, rater_2_challenge)

###Counting variable of "Engagement"
dataset <- dataset %>% mutate (engagement = ifelse(rater_1_engagement == rater_2_engagement, 
                                        rater_2_engagement, rater_3_engagement))
str(dataset)
dataset$engagement <- as.numeric(dataset$engagement)

###Descriptive stats

#Variable "Success in subparts"
hist (dataset$prior_subparts_avg_score_avg)
shapiro.test (dataset$prior_subparts_avg_score_avg) #check normality
qqnorm(dataset$prior_subparts_avg_score_avg, main="")
qqline(dataset$prior_subparts_avg_score_avg, col=2)

#Variable "Prior knowledge"
hist (dataset$prior_knowledge_continuous)
shapiro.test (dataset$prior_knowledge_continuous) #check normality
qqnorm(dataset$prior_knowledge_continuous, main="")
qqline(dataset$prior_knowledge_continuous, col=2)

#Relationships between variables "Success in subparts" and "Prior knowledge"
plot(dataset$prior_knowledge_continuous, dataset$prior_subparts_avg_score_avg)
cor.test(dataset$prior_subparts_avg_score_avg, dataset$prior_knowledge_continuous)

#subparts_avg - check normality - shapiro 
shapiro.test (dataset$prior_subparts_avg_score)
qqnorm(dataset$prior_subparts_avg_score, main="")
qqline(dataset$prior_subparts_avg_score, col=2)

### Inter-rater reliability
install.packages("lpSolve")
library(lpSolve)
install.packages("irr")
library(irr)

##Calculating balance in the rating data (calculate chi square)
table (data_with_grades$julia, data_with_grades$saoirse) #confusion matrix of graders responses
mean(data_with_grades$julia == 0)
mean(data_with_grades$saoirse == 0)

##Calculating percent agreement (should be enough because data is rather balanced)
mean(data_with_grades$julia == data_with_grades$saoirse)

#Sensitivity_for_not-engaged
mean((data_with_grades$julia == 0) & (data_with_grades$saoirse == 0))
data_with_rater_1_engagement_0 <- data_with_grades %>% filter(rater_1_engagement == 0)
mean(data_with_julia_0$rater_2_engagement == 0)

#Sensitivity_for_engaged
mean((data_with_grades$julia == 1) & (data_with_grades$saoirse == 1))
data_with_rater_1_engagement_1 <- dataset %>% filter(rater_1_engagement == 1)
mean(dataset$rater_2_engagement == 1)

##Calculating Cappa
kappa2(data_with_grades[,c("julia","saoirse")], "unweighted")

#Gwet's Agreement Coefficient 1  (REL package) https://rdrr.io/cran/rel/man/gac.html

###Moderation
#mean centering subparts_score (X) and prior_knowkedge (M)
dataset <- dataset %>% 
  mutate (subparts_score_scaled = scale(prior_subparts_avg_score_avg, center=TRUE, scale=FALSE))
dataset <- dataset %>% 
  mutate (prior_knowledge_scaled = scale(prior_knowledge_continuous, center=TRUE, scale=FALSE))

###Modeling

##Model with one predictor (IV)
fitReg <- glm(formula = engagement ~ subparts_score_scaled, family = "binomial", data = dataset)
summary(fitReg)
#McFadden's Pseudo R^2
fitReg_ll.null <- fitReg$null.deviance/-2
fitReg_ll.proposed <- fitReg$deviance/-2
fitReg_Rsq <- (fitReg_ll.null - fitReg_ll.proposed) / fitReg_ll.null
fitReg_Rsq
#p-value
fitReg_p_value <- 1 - pchisq((fitReg$null.deviance - fitReg$deviance), df=1)
fitReg_p_value

##Model with two predictors (IV and moderator)
fitRegWithM <- glm(formula = engagement ~ subparts_score_scaled + prior_knowledge_scaled, family = "binomial", data = dataset)
summary(fitRegWithM)
#McFadden's Pseudo R^2
fitRegWithM_ll.null <- fitRegWithM$null.deviance/-2
fitRegWithM_ll.proposed <- fitRegWithM$deviance/-2
fitRegWithM_Rsq <- (fitRegWithM_ll.null - fitRegWithM_ll.proposed) / fitRegWithM_ll.null
fitRegWithM_Rsq
#p-value
fitRegWithM_p_value <- 1 - pchisq((fitRegWithM$null.deviance - fitRegWithM$deviance), df=1)
fitRegWithM_p_value

##Model with two predictors and moderation
fitMod <- glm(formula = engagement ~ subparts_score_scaled + prior_knowledge_scaled + subparts_score_scaled * prior_knowledge_scaled, family = "binomial", data = dataset)
summary(fitMod)
#McFadden's Pseudo R^2
fitMod_ll.null <- fitMod$null.deviance/-2
fitMod_ll.proposed <- fitMod$deviance/-2
fitMod_Rsq <- (fitMod_ll.null - fitMod_ll.proposed) / fitMod_ll.null
fitMod_Rsq
#p-value
fitMod_p_value <- 1 - pchisq((fitMod$null.deviance - fitMod$deviance), df=1)
fitMod_p_value

##Difference in R-squared (McFadden's Pseudo R^2)
#Moderation model and model with one predictor
fitMod_Rsq - fitReg_Rsq

#Moderation model and model with two predictors
fitMod_Rsq - fitRegWithM_Rsq

#interaction coefficient (b3) in moderation model
names(summary(fitMod))
coef(summary(fitMod))['subparts_score_scaled:prior_knowledge_scaled',1] #b3
#the same
#coef(fitMod)['subparts_score_scaled:prior_knowledge_scaled']

#p-value of interaction coefficient
coef(summary(fitMod))['subparts_score_scaled:prior_knowledge_scaled',4]

